{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b42a25dd",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "\n",
    "As a precursor to multilayer perceptrons,\n",
    "\n",
    "Perceptron is defined as\n",
    "\n",
    "$\\hat{y} = f(w_1x_1 + w_2x_2 + ... w_nx_n + b)$\n",
    "\n",
    "$w$ is a weight vector, $x$ is the input vector, $b$ is a bias, $f$ is an activation function which is the step function with perceptrons\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "61b79c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z: 0.76\n",
      "y_hat: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([0.1, 0.9])\n",
    "w = np.array([0.8, 0.2])\n",
    "b = 0.5\n",
    "\n",
    "z = w @ x + b\n",
    "print(\"z:\", z)\n",
    "\n",
    "def f(z):\n",
    "    return int(z >= 0)\n",
    "\n",
    "print(\"y_hat:\", f(z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3b2c43",
   "metadata": {},
   "source": [
    "we must learn the weights and the bias from the training data using the following formulas\n",
    "\n",
    "$w := w + \\eta(y-\\hat{y})x$\n",
    "\n",
    "$b := b + \\eta(y-\\hat{y})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65cdba0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights: [0.9 0.4]\n",
      "Updated bias: 0.6\n"
     ]
    }
   ],
   "source": [
    "w = np.array([0.8, 0.2])\n",
    "b = 0.5\n",
    "\n",
    "x = np.array([1, 2])\n",
    "y = 1\n",
    "y_hat = 0\n",
    "eta = 0.1\n",
    "\n",
    "w_new = w + eta * (y - y_hat) * x\n",
    "b = b + eta * (y - y_hat) \n",
    "print(\"Updated weights:\", w_new)\n",
    "print(\"Updated bias:\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95364e15",
   "metadata": {},
   "source": [
    "#### Perceptron from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0123a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, learning_rate=0.1, epochs=10):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def activation(self, z):\n",
    "        return np.where(z >= 0, 1, 0)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Initialize weights and bias to zero\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0.0\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(self.epochs):\n",
    "            for i in range(n_samples):\n",
    "                # Linear combination and prediction\n",
    "                z = np.dot(X[i], self.weights) + self.bias\n",
    "                y_pred = self.activation(z)\n",
    "\n",
    "                # Perceptron update rule\n",
    "                error = y[i] - y_pred\n",
    "                self.weights += self.learning_rate * error * X[i]\n",
    "                self.bias += self.learning_rate * error\n",
    "\n",
    "    def predict(self, X):\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        return self.activation(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a067b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([\n",
    "    [1, 1],\n",
    "    [1, 0],\n",
    "    [0, 1],\n",
    "    [0, 0]\n",
    "])\n",
    "\n",
    "y = np.array([1, 1, 1, 0]) # OR logic gate\n",
    "\n",
    "model = Perceptron(learning_rate=0.01, epochs=10)\n",
    "model.fit(X, y)\n",
    "predictions = model.predict(X)\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc3efb5",
   "metadata": {},
   "source": [
    "#### Limitation:\n",
    "\n",
    "Does not work for non-linear decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e908c442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "y = np.array([0, 1, 1, 0]) # XOR, nonlinear, should not work\n",
    "\n",
    "model = Perceptron(learning_rate=0.1, epochs=50)\n",
    "model.fit(X, y)\n",
    "predictions = model.predict(X)\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77660c7c",
   "metadata": {},
   "source": [
    "A single-layer perceptron can only handle linearly separable problems, which makes it too limited for most real tasks. To model more complex, nonlinear patterns, we add hidden layers and use smooth activation functions instead of a hard threshold. This leads to the multilayer perceptron (MLP), where stacked layers learn richer representations and can be trained efficiently using backpropagation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

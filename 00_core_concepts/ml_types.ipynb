{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fc6eac4",
   "metadata": {},
   "source": [
    "# Different Types of ML\n",
    "\n",
    "This notebook will explain different types of ML. \n",
    "\n",
    "1) Supervised Learning\n",
    "\n",
    "2) Unsupervised Learning\n",
    "\n",
    "3) Reinforcement Learning\n",
    "\n",
    "4) Semi-Supervised Learning\n",
    "\n",
    "5) Self-Supervised Learning\n",
    "\n",
    "6) Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3350a249",
   "metadata": {},
   "source": [
    "### 1) Supervised Learning\n",
    "\n",
    "Supervised learning trains a model on labeled data, meaning each input comes with a known target value.\n",
    "After training, the model can predict labels for new, unseen data.\n",
    "\n",
    "The label can be:\n",
    "\n",
    "- Continuous → regression\n",
    "\n",
    "- Categorical → classification\n",
    "\n",
    "Simple examples:\n",
    "\n",
    "- Predicting a house price from its size (regression)\n",
    "\n",
    "- Predicting whether a student will pass based on study time (classification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f6e2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A house with size 60 m^2 is predicted to cost:  [180.] k\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[30], [50], [70]]) # house size (m^2)\n",
    "y = np.array([90, 150, 210]) # house price (k euros) \n",
    "\n",
    "model = LinearRegression().fit(X, y)\n",
    "print(\"A house with size 60 m^2 is predicted to cost: \", model.predict([[60]]), \"k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "141418f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student with 4 hours study time predicted to pass: [1]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1], [5], [3]]) # study time (h)\n",
    "y = np.array([0, 1, 1]) # pass = 1, fail = 0\n",
    "\n",
    "model = LogisticRegression().fit(X, y)\n",
    "print(\"Student with 4 hours study time predicted to pass:\", model.predict([[4]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9180640b",
   "metadata": {},
   "source": [
    "### 2) Unsupervised Learning\n",
    "\n",
    "The model is trained on unlabeled data. There are no target values.\n",
    "The goal is to discover structure or patterns in the data on its own.\n",
    "\n",
    "Common tasks include:\n",
    "\n",
    "- Clustering → grouping similar data points\n",
    "\n",
    "- Dimensionality reduction → compressing data while keeping important structure\n",
    "\n",
    "Simple examples:\n",
    "\n",
    "- Grouping customers based on their purchasing behavior (clustering)\n",
    "\n",
    "- Compressing 2D points onto a 1D line while preserving variance (good if you want to visualize higher dimensional data) (PCA dimension reduction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34aead91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# for example customer data, features could be revenue per person and time spent on the website\n",
    "X = np.array([[1,1], [1.2,1.1], [5,5], [5.2,5.1]])\n",
    "\n",
    "kmeans = KMeans(n_clusters=2).fit(X) # we want 2 clusters\n",
    "print(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e87a194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.41421356]\n",
      " [ 0.        ]\n",
      " [ 1.41421356]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = np.array([[1,2], [2,3], [3,4]])\n",
    "pca = PCA(n_components=1).fit_transform(X)\n",
    "print(pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51771b4",
   "metadata": {},
   "source": [
    "### 3) Reinforcement Learning\n",
    "\n",
    "In reinforcement learning, an agent learns by interacting with an environment.\n",
    "There are no labeled examples. Instead, the agent receives rewards or penalties based on its actions.\n",
    "\n",
    "The goal is to learn a strategy for choosing actions that maximizes long-term reward. This is called the policy.\n",
    "\n",
    "Examples:\n",
    "\n",
    "- A maze agent: It starts at the entrance, tries different paths, gets a \"+1\" for reaching the exit and small penalties for hitting walls or wandering too long. Over time it discovers the fastest path.\n",
    "\n",
    "- Chess engine: The agent plays millions of games against itself.\n",
    "Winning gives a positive reward, losing gives a negative reward.\n",
    "It gradually learns strategies, tactics, and opening principles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386bec44",
   "metadata": {},
   "source": [
    "### 4) Semi-Supervised Learning\n",
    "\n",
    "Semi-supervised learning is used when you have a small amount of labeled data and a large amount of unlabeled data.\n",
    "The idea is to use the labeled samples to guide the model, and then let the model learn structure from the unlabeled data to improve its predictions.\n",
    "\n",
    "I think of it as a mix of **supervised** and **unsupervised** learning. The model doesn't automatically learn labels, but rather uses patterns in the unlabeled data (e.g., clusters or similarity) to make the limited labels more informative.\n",
    "\n",
    "Examples:\n",
    "\n",
    "- Classifying handwritten digits with only a few labeled images:\n",
    "If you label 50 digits out of 10,000, the model can use the unlabeled ones to discover digit-shaped clusters and improve accuracy.\n",
    "\n",
    "- Detecting spam emails when only a small portion are labeled:\n",
    "A few labeled spam/ham emails guide the model; the large pool of unlabeled emails helps it learn common structures in email text.\n",
    "\n",
    "- Labeling images of cats vs. dogs when you only labeled 100 out of 5,000:\n",
    "The model groups similar images together, then spreads the known labels to nearby unlabeled ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f22ff2",
   "metadata": {},
   "source": [
    "### 5) Self-Supervised Learning\n",
    "\n",
    "Self-supervised learning is a way for a model to learn useful representations without using any human-provided labels at all.\n",
    "It's still supervised learning in the sense that the model learns from a target, but those targets come directly from the data rather than from humans. Self-supervised learning is especially powerful when labeled data is expensive or rare, but unlabeled data (images, text, audio, video) is abundant (like NLP and CV).\n",
    "\n",
    "Examples:\n",
    "\n",
    "- Predicting the missing part of an image:\n",
    "    - Hide a patch of an image and ask the model to predict what should be there.\n",
    "By doing this repeatedly, the model learns shapes, textures, and object structure.\n",
    "\n",
    "- Colorizing black-and-white images:\n",
    "    - Convert a color image to grayscale and train the model to predict the missing colors.\n",
    "The model must understand objects (e.g., sky is blue, grass is green) to do this well.\n",
    "\n",
    "- Predicting the next frame in a video:\n",
    "    - Given earlier frames, predict what comes next.\n",
    "The model learns motion, object permanence, and temporal structure.\n",
    "\n",
    "- Learning sentence embeddings by predicting masked words (BERT):\n",
    "    - Mask 15% of words in a sentence and ask the model to guess them.\n",
    "The model learns grammar, meaning, and relationships between words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cd3114",
   "metadata": {},
   "source": [
    "### 6) Transfer Learning\n",
    "\n",
    "Transfer learning is used when you want to take a model that has already learned something useful on one task and reuse that knowledge to solve a different task. This reduces training costs and data requirements.\n",
    "\n",
    "Typically, you either:\n",
    "\n",
    "- Freeze the pretrained model and only train a small classifier head\n",
    "\n",
    "- Fine-tune the whole model on your new dataset with a smaller learning rate\n",
    "\n",
    "Examples:\n",
    "\n",
    "- Using a vision model pretrained on everyday photos to classify rare animals:\n",
    "    - Even if you have only 200 labeled examples of a rare animal species, the pretrained model already understands visual concepts like fur texture, limb shapes, and colors, making learning much faster and more accurate.\n",
    "\n",
    "- Adapting a large language model (like BERT) for sentiment analysis:\n",
    "    - BERT learns general language structure during pretraining.\n",
    "Fine-tuning it on a small labeled dataset (positive/negative reviews) gives strong performance with minimal data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

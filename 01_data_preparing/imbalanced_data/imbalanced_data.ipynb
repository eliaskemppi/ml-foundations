{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9842c061",
   "metadata": {},
   "source": [
    "# Imbalanced data\n",
    "\n",
    "Imagine we have data where 95% of samples belong to class 1 and only 5% to class 2.\n",
    "Now imagine a classifier that always predicts class 1. It would still achieve 95% accuracy while having no real predictive power.\n",
    "\n",
    "This situation is called class imbalance, and it’s common in real-world problems such as fraud detection, disease diagnosis, or rare event prediction.\n",
    "When one class heavily dominates the other, a model can appear to perform well by focusing only on the majority class while ignoring the minority.\n",
    "\n",
    "Therefore, accuracy can be a misleading metric for imbalanced data. Instead, it’s better to evaluate models using precision, recall, and the F1-score, which better reflect how well the minority class is identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dd5d757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eae76d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression without resampling:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97      1181\n",
      "           1       0.61      0.25      0.35        69\n",
      "\n",
      "    accuracy                           0.95      1250\n",
      "   macro avg       0.78      0.62      0.66      1250\n",
      "weighted avg       0.94      0.95      0.94      1250\n",
      "\n",
      "Random Forest without resampling\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      1181\n",
      "           1       0.75      0.52      0.62        69\n",
      "\n",
      "    accuracy                           0.96      1250\n",
      "   macro avg       0.86      0.76      0.80      1250\n",
      "weighted avg       0.96      0.96      0.96      1250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=5000,\n",
    "    n_features=10,\n",
    "    n_classes=2,\n",
    "    weights=[0.95, 0.05], # 5% minority class\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "lr = LogisticRegression(max_iter=2000)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "print(\"Logistic Regression without resampling:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(\"Random Forest without resampling\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec29ea",
   "metadata": {},
   "source": [
    "The random forest seems to handle the imbalance decently well. It still detects some minority samples.  \n",
    "Logistic regression, on the other hand, struggles: its recall and F1 for the 5% class are very low.  \n",
    "Let's see if resampling can help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd656d6d",
   "metadata": {},
   "source": [
    "## Resampling Methods:\n",
    "\n",
    "### Random Over Sampler\n",
    "\n",
    "This method randomly **duplicates existing minority samples** until the classes are balanced.\n",
    "\n",
    "**Pros:**\n",
    "- Simple and fast.\n",
    "- Keeps all original data.\n",
    "\n",
    "**Cons:**\n",
    "- Can lead to **overfitting**, since the model sees identical samples multiple times.\n",
    "- Doesn’t add new information.\n",
    "\n",
    "Below we retrain both models after oversampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4b042d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.85      0.91      1181\n",
      "           1       0.22      0.72      0.34        69\n",
      "\n",
      "    accuracy                           0.85      1250\n",
      "   macro avg       0.60      0.79      0.63      1250\n",
      "weighted avg       0.94      0.85      0.88      1250\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      1181\n",
      "           1       0.69      0.61      0.65        69\n",
      "\n",
      "    accuracy                           0.96      1250\n",
      "   macro avg       0.83      0.80      0.81      1250\n",
      "weighted avg       0.96      0.96      0.96      1250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(sampling_strategy='minority', random_state=42)\n",
    "X_train_ros, y_train_ros = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Logistic Regression on oversampled data\n",
    "lr.fit(X_train_ros, y_train_ros)\n",
    "y_pred_lr_ros = lr.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_lr_ros))\n",
    "\n",
    "# Random Forest on oversampled data\n",
    "rf.fit(X_train_ros, y_train_ros)\n",
    "y_pred_rf_ros = rf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_rf_ros))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c3df17",
   "metadata": {},
   "source": [
    "### Random Under Sampler\n",
    "\n",
    "This method randomly **removes majority-class samples** to balance the dataset.\n",
    "\n",
    "**Pros:**\n",
    "- Quick to run.\n",
    "- Prevents overfitting to the majority class.\n",
    "\n",
    "**Cons:**\n",
    "- **Loses information** from the majority class.\n",
    "- The model may generalize worse if too much data is dropped.\n",
    "\n",
    "Results below show (marginally) higher recall for the minority class but lower overall accuracy, which fits expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb13c4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.85      0.91      1181\n",
      "           1       0.22      0.75      0.34        69\n",
      "\n",
      "    accuracy                           0.84      1250\n",
      "   macro avg       0.60      0.80      0.63      1250\n",
      "weighted avg       0.94      0.84      0.88      1250\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.91      0.95      1181\n",
      "           1       0.32      0.72      0.45        69\n",
      "\n",
      "    accuracy                           0.90      1250\n",
      "   macro avg       0.65      0.82      0.70      1250\n",
      "weighted avg       0.95      0.90      0.92      1250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler(sampling_strategy='majority', random_state=42)\n",
    "X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# Logistic Regression on undersampled data\n",
    "lr.fit(X_train_rus, y_train_rus)\n",
    "y_pred_lr_rus = lr.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_lr_rus))\n",
    "\n",
    "# Random Forest on undersampled data\n",
    "rf.fit(X_train_rus, y_train_rus)\n",
    "y_pred_rf_rus = rf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_rf_rus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26346ae",
   "metadata": {},
   "source": [
    "### SMOTE (Synthetic Minority Oversampling Technique)\n",
    "\n",
    "Instead of duplicating samples, SMOTE creates **synthetic examples** by interpolating between existing minority samples and their nearest neighbors.\n",
    "\n",
    "**Pros:**\n",
    "- Adds variety to minority data → less overfitting than plain oversampling.\n",
    "- Often improves recall and smooths the decision boundary.\n",
    "\n",
    "**Cons:**\n",
    "- Can introduce **noisy or unrealistic samples** if classes overlap.\n",
    "- Doesn’t work well with categorical features.\n",
    "\n",
    "Let’s see how it performs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73ad4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.86      0.92      1181\n",
      "           1       0.24      0.74      0.36        69\n",
      "\n",
      "    accuracy                           0.85      1250\n",
      "   macro avg       0.61      0.80      0.64      1250\n",
      "weighted avg       0.94      0.85      0.89      1250\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97      1181\n",
      "           1       0.54      0.62      0.58        69\n",
      "\n",
      "    accuracy                           0.95      1250\n",
      "   macro avg       0.76      0.80      0.78      1250\n",
      "weighted avg       0.95      0.95      0.95      1250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Resample training data using SMOTE\n",
    "smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Logistic Regression on SMOTE data\n",
    "lr.fit(X_train_smote, y_train_smote)\n",
    "y_pred_lr_smote = lr.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_lr_smote))\n",
    "\n",
    "# Random Forest on SMOTE data\n",
    "rf.fit(X_train_smote, y_train_smote)\n",
    "y_pred_rf_smote = rf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_rf_smote))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2588fe6",
   "metadata": {},
   "source": [
    "## Algorithm-Level Method: Class Weights\n",
    "\n",
    "Instead of changing the data, we can adjust the algorithm itself to pay more attention to the minority class.\n",
    "This is done using class weights, which assign higher penalties to misclassifications of minority samples.\n",
    "In other words, errors on rare classes \"cost\" more during training, encouraging the model to learn to recognize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2b269ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with class_weight='balanced':\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.85      0.91      1181\n",
      "           1       0.22      0.72      0.34        69\n",
      "\n",
      "    accuracy                           0.84      1250\n",
      "   macro avg       0.60      0.79      0.63      1250\n",
      "weighted avg       0.94      0.84      0.88      1250\n",
      "\n",
      "Random Forest with class_weight='balanced':\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      1181\n",
      "           1       0.73      0.52      0.61        69\n",
      "\n",
      "    accuracy                           0.96      1250\n",
      "   macro avg       0.85      0.76      0.80      1250\n",
      "weighted avg       0.96      0.96      0.96      1250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression with class weights\n",
    "lr_bal = LogisticRegression(max_iter=2000, class_weight='balanced', random_state=42)\n",
    "lr_bal.fit(X_train, y_train)\n",
    "y_pred_lr_bal = lr_bal.predict(X_test)\n",
    "\n",
    "print(\"Logistic Regression with class_weight='balanced':\")\n",
    "print(classification_report(y_test, y_pred_lr_bal))\n",
    "\n",
    "# Random Forest with class weights\n",
    "rf_bal = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "rf_bal.fit(X_train, y_train)\n",
    "y_pred_rf_bal = rf_bal.predict(X_test)\n",
    "\n",
    "print(\"Random Forest with class_weight='balanced':\")\n",
    "print(classification_report(y_test, y_pred_rf_bal))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476b86fa",
   "metadata": {},
   "source": [
    "Logistic regression’s results are very similar to oversampling, while Random Forest’s performance is nearly unchanged, again showing that tree ensembles already handle imbalance fairly well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfb4df4",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Resampling helped logistic regression a lot: its recall for the minority class jumped from 0.25 to over 0.7 with any resampling method.  \n",
    "Random forest didn't improve significantly, which makes sense since tree ensembles already handle imbalance relatively well due to their sampling and splitting mechanisms.\n",
    "\n",
    "In general:\n",
    "- **Oversampling / SMOTE** increase recall but may reduce precision.\n",
    "- **Undersampling** increases recall more but often reduces overall accuracy.\n",
    "- **Class weights** offer a simple algorithm-level alternative, letting the model focus more on minority samples without changing the data.\n",
    "- For complex models like RandomForest, class weights or tuning the decision threshold may be more efficient than resampling.\n",
    "\n",
    "Overall, this shows *why and when* resampling techniques are useful, especially for simpler, linear models on strongly imbalanced data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616f8ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

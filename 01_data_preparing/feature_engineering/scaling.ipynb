{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13c70ca7",
   "metadata": {},
   "source": [
    "# Feature Scaling\n",
    "\n",
    "Different features in a dataset can have very different ranges. For example, \"income\" might range from 20,000 to 200,000, while \"age\" ranges from 18 to 70.\n",
    "Many algorithms (like logistic regression, k-nearest neighbors, or gradient descent-based models) perform poorly when features are on very different scales, because large-valued features dominate.\n",
    "\n",
    "That's why feature scaling is an important preprocessing step.\n",
    "I'll present two common methods: Min-Max scaling (0-1 scaling) and Standardization (normalizing to mean 0, std 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac032868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unscaled:\n",
      "[[3.74540119e+03 1.09507143e+01 7.31993942e-01]\n",
      " [5.98658484e+03 1.01560186e+01 1.55994520e-01]\n",
      " [5.80836122e+02 1.08661761e+01 6.01115012e-01]\n",
      " [7.08072578e+03 1.00205845e+01 9.69909852e-01]\n",
      " [8.32442641e+03 1.02123391e+01 1.81824967e-01]]\n",
      "1st feature: min 55.22117123602399 max 9900.538501042633\n",
      "2st feature: min 10.005061583846219 max 10.985650454110601\n",
      "3st feature: min 0.006952130531190703 max 0.9699098521619943\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create a dataset with 3 features, one of which has a much larger scale\n",
    "X = np.random.rand(100, 3)\n",
    "X[:, 0] = X[:, 0] * 10000\n",
    "X[:, 1] = X[:, 1] + 10\n",
    "\n",
    "print(\"Unscaled:\")\n",
    "print(X[:5])\n",
    "print(\"1st feature: min\", min(X[:, 0]), \"max\", max(X[:, 0]))\n",
    "print(\"2st feature: min\", min(X[:, 1]), \"max\", max(X[:, 1]))\n",
    "print(\"3st feature: min\", min(X[:, 2]), \"max\", max(X[:, 2]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73db55c",
   "metadata": {},
   "source": [
    "## Min-max scaling\n",
    "\n",
    "$x'= \\frac{min(x)}{(max(x)-min(x))}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba455943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Max scaled:\n",
      "[[0.37481575 0.96437228 0.75293213]\n",
      " [0.60245531 0.15394531 0.15477563]\n",
      " [0.0533873  0.87816065 0.61701866]\n",
      " [0.71358844 0.01583019 1.        ]\n",
      " [0.83991251 0.21138066 0.1815997 ]]\n",
      "1st feature: min 0.0 max 1.0\n",
      "2st feature: min 0.0 max 1.0\n",
      "3st feature: min 0.0 max 1.0\n"
     ]
    }
   ],
   "source": [
    "X_minmax = (X-X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "\n",
    "print(\"Min-Max scaled:\")\n",
    "print(X_minmax[:5])\n",
    "\n",
    "print(\"1st feature: min\", min(X_minmax[:, 0]), \"max\", max(X_minmax[:, 0]))\n",
    "print(\"2st feature: min\", min(X_minmax[:, 1]), \"max\", max(X_minmax[:, 1]))\n",
    "print(\"3st feature: min\", min(X_minmax[:, 2]), \"max\", max(X_minmax[:, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d8ceff",
   "metadata": {},
   "source": [
    "## Standardizing\n",
    "\n",
    "$x'= \\frac{x-\\mu}{\\sigma}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58f0455b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized:\n",
      "[[-0.35958026  1.4198383   0.82216804]\n",
      " [ 0.42811633 -1.15479213 -1.18711031]\n",
      " [-1.47181267  1.14595419  0.36561862]\n",
      " [ 0.81266807 -1.59356748  1.65209819]\n",
      " [ 1.24978474 -0.97232681 -1.09700508]]\n",
      "1st feature: min: -1.6565476851937493  max: 1.8037322538398184\n",
      "2st feature: min: -1.6438581271334913  max: 1.5330233528481956\n",
      "3st feature: min: -1.7070199406136317  max: 1.652098194094035\n"
     ]
    }
   ],
   "source": [
    "X_standard = (X-X.mean(axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "print(\"Standardized:\")\n",
    "print(X_standard[:5])\n",
    "\n",
    "print(\"1st feature: min:\", min(X_standard[:, 0]), \" max:\", max(X_standard[:, 0]))\n",
    "print(\"2st feature: min:\", min(X_standard[:, 1]), \" max:\", max(X_standard[:, 1]))\n",
    "print(\"3st feature: min:\", min(X_standard[:, 2]), \" max:\", max(X_standard[:, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5fa95d",
   "metadata": {},
   "source": [
    "# Example: Support Vector Machine Classification\n",
    "\n",
    "Let's try SVM with unscaled and standardized data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1cfc6cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Make synthetic data with features on very different scales\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=3,\n",
    "    n_informative=3,\n",
    "    n_redundant=0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Artificially rescale one feature to be huge\n",
    "X[:, 0] = X[:, 0] * 10000   # one dominant feature\n",
    "X[:, 1] = X[:, 1] * 0.01   # one tiny feature\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "334a3e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM (Unscaled):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.84      0.87       140\n",
      "           1       0.81      0.89      0.85       110\n",
      "\n",
      "    accuracy                           0.86       250\n",
      "   macro avg       0.86      0.86      0.86       250\n",
      "weighted avg       0.86      0.86      0.86       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# SVM on unscaled data\n",
    "svm_unscaled = SVC()\n",
    "svm_unscaled.fit(X_train, y_train)\n",
    "y_pred_unscaled = svm_unscaled.predict(X_test)\n",
    "\n",
    "print(\"SVM (Unscaled):\")\n",
    "print(classification_report(y_test, y_pred_unscaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2c6c3bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM (Standardized):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.91      0.93       140\n",
      "           1       0.90      0.95      0.92       110\n",
      "\n",
      "    accuracy                           0.93       250\n",
      "   macro avg       0.93      0.93      0.93       250\n",
      "weighted avg       0.93      0.93      0.93       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "svm_scaled = SVC()\n",
    "svm_scaled.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = svm_scaled.predict(X_test_scaled)\n",
    "\n",
    "print(\"SVM (Standardized):\")\n",
    "print(classification_report(y_test, y_pred_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55175ffc",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Scaling has a clear impact on SVM performance. On unscaled data, the model struggles to balance contributions from features with different ranges, resulting in lower accuracy, precision and recall.\n",
    "After standardization, all features contribute equally, and the SVM achieves noticeably higher accuracy and balanced F1-scores for both classes.\n",
    "\n",
    "Key takeaway: SVMs (and other distance-based models) almost always require feature scaling to perform optimally."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

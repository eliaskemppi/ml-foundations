{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13ed3475",
   "metadata": {},
   "source": [
    "# Handling Missing Values\n",
    "\n",
    "Missing data is extremely common in real-world datasets. How you handle it can significantly affect model performance and bias.\n",
    "Before choosing a method, it’s important to understand why data is missing:\n",
    "\n",
    "- **MCAR** - Missing Completely At Random\n",
    "\n",
    "- **MAR** - Missing At Random\n",
    "\n",
    "- **MNAR** - Missing Not At Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f2b1a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  target\n",
      "0  -0.562931  -1.122449  -1.157244  -2.470523   0.496637  -0.326114       0\n",
      "1   0.645020   2.236070  -1.062233   1.627740   0.366363   0.852120       1\n",
      "2   2.737576   1.427951   1.053366  -1.467892   2.862033  -1.535227       1\n",
      "3   0.231788   2.540746  -1.967838   1.777564   0.296485   1.419086       1\n",
      "4   0.636727  -1.257201   0.334326  -1.136824  -1.973597  -1.997091       0\n",
      "percentage of missing values per feature\n",
      " feature_0    0.154\n",
      "feature_1    0.156\n",
      "feature_2    0.152\n",
      "feature_3    0.140\n",
      "feature_4    0.170\n",
      "feature_5    0.152\n",
      "target       0.162\n",
      "dtype: float64\n",
      "\n",
      "Final MCAR data\n",
      "    feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  target\n",
      "0  -0.562931  -1.122449  -1.157244  -2.470523        NaN  -0.326114     0.0\n",
      "1   0.645020        NaN  -1.062233   1.627740   0.366363   0.852120     1.0\n",
      "2   2.737576   1.427951   1.053366        NaN   2.862033  -1.535227     1.0\n",
      "3   0.231788   2.540746  -1.967838   1.777564   0.296485   1.419086     NaN\n",
      "4   0.636727  -1.257201   0.334326  -1.136824  -1.973597  -1.997091     0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# 1. Generate synthetic dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=500,\n",
    "    n_features=6,\n",
    "    n_informative=4,\n",
    "    n_redundant=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(6)])\n",
    "df[\"target\"] = y\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# 2. Introduce MCAR missingness\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "missing_rate = 0.15  # 15% missing entries\n",
    "mask = rng.random(df.shape) < missing_rate\n",
    "\n",
    "df_mcar = df.copy()\n",
    "df_mcar[mask] = np.nan\n",
    "\n",
    "print(\"percentage of missing values per feature\\n\", df_mcar.isna().mean())\n",
    "\n",
    "print(\"\\nFinal MCAR data\\n\", df_mcar.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dbf86f",
   "metadata": {},
   "source": [
    "### Case 1: Missing Completely At Random (MCAR)\n",
    "\n",
    "When data is Missing Completely At Random, the probability of a value being missing is unrelated to the data itself.\n",
    "Example: a sensor randomly malfunctions for no reason.\n",
    "\n",
    "For MCAR we can:\n",
    "\n",
    "#### A) Drop rows with missing values\n",
    "\n",
    "Simple, but can remove a huge amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e24ead6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156\n"
     ]
    }
   ],
   "source": [
    "df_dropped = df_mcar.dropna()\n",
    "print(df_dropped.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e29b9fd",
   "metadata": {},
   "source": [
    "Dropping rows removed over two-thirds of the data → usually not ideal because losing so many samples can reduce statistical power and remove important patterns.\n",
    "\n",
    "#### B) Simple imputation (median/mean)\n",
    "\n",
    "Keeps all data, but shrinks variance and may slightly reduce model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f46213eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  target\n",
      "0  -0.562931  -1.122449  -1.157244  -2.470523  -0.435368  -0.326114     0.0\n",
      "1   0.645020  -0.039903  -1.062233   1.627740   0.366363   0.852120     1.0\n",
      "2   2.737576   1.427951   1.053366  -0.030986   2.862033  -1.535227     1.0\n",
      "3   0.231788   2.540746  -1.967838   1.777564   0.296485   1.419086     0.0\n",
      "4   0.636727  -1.257201   0.334326  -1.136824  -1.973597  -1.997091     0.0\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "df_median = df_mcar.fillna(df_mcar.median())\n",
    "print(df_median.head())\n",
    "print(df_median.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d886b903",
   "metadata": {},
   "source": [
    "### Case 2: Missing At Random (MAR)\n",
    "\n",
    "MAR means missingness depends on other observed variables, not on the missing value itself.\n",
    "Example:\n",
    "\n",
    "- Younger people are less likely to report income\n",
    "\n",
    "- Missingness depends on age, but not on income itself\n",
    "\n",
    "Using a single global median would bias results (e.g., inflating incomes for young people).\n",
    "\n",
    "For MAR, better methods are:\n",
    "\n",
    "#### A) KNN Imputation\n",
    "\n",
    "Matches each row with similar rows and uses neighbor values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "104e409e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  target\n",
      "0  -0.562931  -1.122449  -1.157244  -2.470523   0.024339  -0.326114     0.0\n",
      "1   0.645020   1.723768  -1.062233   1.627740   0.366363   0.852120     1.0\n",
      "2   2.737576   1.427951   1.053366  -0.136634   2.862033  -1.535227     1.0\n",
      "3   0.231788   2.540746  -1.967838   1.777564   0.296485   1.419086     1.0\n",
      "4   0.636727  -1.257201   0.334326  -1.136824  -1.973597  -1.997091     0.0\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df_knn = pd.DataFrame(imputer.fit_transform(df_mcar), columns=df.columns)\n",
    "\n",
    "print(df_knn.head())\n",
    "print(df_knn.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bde6f6",
   "metadata": {},
   "source": [
    "#### B) Multiple Imputation (MICE / IterativeImputer)\n",
    "\n",
    "Predicts each feature using the others in a round-robin fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24e84534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5    target\n",
      "0  -0.562931  -1.122449  -1.157244  -2.470523   0.496544  -0.326114  0.000000\n",
      "1   0.645020   2.238272  -1.062233   1.627740   0.366363   0.852120  1.000000\n",
      "2   2.737576   1.427951   1.053366  -1.468055   2.862033  -1.535227  1.000000\n",
      "3   0.231788   2.540746  -1.967838   1.777564   0.296485   1.419086  0.590913\n",
      "4   0.636727  -1.257201   0.334326  -1.136824  -1.973597  -1.997091  0.000000\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "df_mice = pd.DataFrame(imputer.fit_transform(df_mcar), columns=df.columns)\n",
    "\n",
    "print(df_mice.head())\n",
    "print(df_mice.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa142a1",
   "metadata": {},
   "source": [
    "Both KNN and MICE work well for **MCAR** and **MAR**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e691b9bb",
   "metadata": {},
   "source": [
    "### Case 3: Missing Not At Random (MNAR)\n",
    "\n",
    "MNAR means the missingness depends on the value that is missing.\n",
    "Example:\n",
    "\n",
    "- People with very high income avoid reporting it\n",
    "\n",
    "In this case, imputation alone cannot fix the bias. The unobserved data is fundamentally different from the observed data.\n",
    "\n",
    "Here's what we can do:\n",
    "\n",
    "#### A) Add missing-indicator features\n",
    "\n",
    "Allows the model to use the missingness itself as information.\n",
    "\n",
    "#### B) Sensitivity analysis\n",
    "\n",
    "Train multiple models under different assumptions:\n",
    "\n",
    "assuming missing values are\n",
    "\n",
    "10% lower\n",
    "\n",
    "20% lower\n",
    "\n",
    "40% lower, etc.\n",
    "\n",
    "If conclusions stay stable → your result is robust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e756625e",
   "metadata": {},
   "source": [
    "(For this mini project, I left any unrealistic target values (like 0.590913) as-is for simplicity. In a real-world scenario, I would clean or correct these values, but here I ignored them to keep the example straightforward.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5203bb2b",
   "metadata": {},
   "source": [
    "# Quantization\n",
    "\n",
    "Quantization speeds up LLM inference by using fewer bits to represent model weights. This reduces both computation time and memory usage, at the cost of some loss in accuracy.\n",
    "\n",
    "By default, weights are stored as 32-bit floating point numbers (FP32). Quantization approximates these values using lower-precision formats such as 16-bit floats (FP16), 8-bit integers (INT8), or 4-bit integers (INT4). Fewer bits mean less precise weight values, which introduces approximation errors and can slightly degrade model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a5c3c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6a4dd2",
   "metadata": {},
   "source": [
    "#### Original weight dtype:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c1fc7aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "333941784\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilgpt2\"\n",
    "\n",
    "model1 = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "for name, param in model1.named_parameters():\n",
    "    print(param.dtype)\n",
    "    break\n",
    "\n",
    "print(model1.get_memory_footprint())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c673bd",
   "metadata": {},
   "source": [
    "#### FP16:\n",
    "\n",
    "you can change the dtype **to a float** like as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "356697dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n",
      "170116620\n"
     ]
    }
   ],
   "source": [
    "model2 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=torch.float16 # this line quantizes the model to float16\n",
    ")\n",
    "\n",
    "for name, param in model2.named_parameters():\n",
    "    print(param.dtype)\n",
    "    break\n",
    "\n",
    "print(model2.get_memory_footprint())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c449cf9e",
   "metadata": {},
   "source": [
    "#### 8-bit int:\n",
    "\n",
    "We canâ€™t directly change the type like above because integer weights cannot represent real-valued parameters used in neural networks. Instead, we use a `quantization_config` with BitsAndBytes, which stores weights as INT8 along with scale factors that map them back to floating-point values during computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a8c62e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127649292\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "model3 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config # add quantization config here\n",
    ")\n",
    "\n",
    "print(model3.get_memory_footprint())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7860def",
   "metadata": {},
   "source": [
    "We can see that the memory footprint got reduced a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfcf2e1",
   "metadata": {},
   "source": [
    "#### Small example and comparison:\n",
    "\n",
    "Lets see how they behave with an example prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f61e8634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time of war, the United States was the only country in the world to have a military presence. The United States was the only country in the world to\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Once upon a time\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model1.device)\n",
    "\n",
    "out = model1.generate(**inputs, max_new_tokens=30)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b8d065c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time of war, the United States was the only country in the world to have a military presence. The United States was the only country in the world to\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model3.device)\n",
    "\n",
    "out = model3.generate(**inputs, max_new_tokens=30)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce3d5ed",
   "metadata": {},
   "source": [
    "The responses produced by the FP32 and INT8 models are identical for this example prompt. For short prompts, the small numerical differences introduced by INT8 quantization often do not affect the final output.\n",
    "\n",
    "In addition, the model used here (DistilGPT-2) is very small by modern standards. In larger models, or with longer and more complex prompts, small differences caused by quantization are more likely to become noticeable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51fb1def",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "While BoW and TF-IDF produce a single vector representation for an entire text document, word embeddings assign vector representations to individual words (or tokens; we say \"words\" here for simplicity).\n",
    "\n",
    "These word vectors can then be fed sequentially into models such as RNNs or processed as a sequence by Transformers, allowing the model to take word order into account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a6d3f8",
   "metadata": {},
   "source": [
    "## Static Word Embeddings (example: Word2Vec)\n",
    "\n",
    "Each word is assigned a single, fixed vector representation. Words with similar meanings tend to have similar vectors. A major limitation is that homonyms (words with multiple meanings) share the same vector regardless of context, even though their meanings may differ.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95a98aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Fake embedding dictionary\n",
    "emb = {\n",
    "    \"i\": np.array([1.0, 0.0]),\n",
    "    \"love\": np.array([0.8, 0.6]),\n",
    "    \"hate\": np.array([-0.8, 0.6]),\n",
    "    \"nlp\": np.array([0.9, 0.1]),\n",
    "    \"machine\": np.array([0.7, 0.7]),\n",
    "    \"learning\": np.array([0.6, 0.8]),\n",
    "    \"exams\": np.array([-0.9, 0.2])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f074069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 0. ],\n",
       "       [0.8, 0.6],\n",
       "       [0.9, 0.1]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def s2emb(sentence, embeddings):\n",
    "    vectors = []\n",
    "    for word in sentence.lower().split():\n",
    "        vectors.append(embeddings[word])\n",
    "    return np.array(vectors)\n",
    "\n",
    "sentence = \"I love NLP\"\n",
    "s2emb(sentence, emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894e9aeb",
   "metadata": {},
   "source": [
    "These vectors can then be given to a RNN etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf30a4e",
   "metadata": {},
   "source": [
    "#### Example of concructing Word2Vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "493bedd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"i love nlp\",\n",
    "    \"i love machine learning\",\n",
    "    \"nlp loves machine learning\",\n",
    "    \"i hate exams\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "836d0f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exams': 0, 'hate': 1, 'i': 2, 'learning': 3, 'love': 4, 'loves': 5, 'machine': 6, 'nlp': 7}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Tokenize and map to indices\n",
    "\n",
    "tokenized = [s.split() for s in sentences]\n",
    "\n",
    "vocab = sorted(set(word for sent in tokenized for word in sent))\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "\n",
    "print(word2idx)\n",
    "\n",
    "vocab_size = len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "138af083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 4),\n",
       " (4, 2),\n",
       " (4, 7),\n",
       " (7, 4),\n",
       " (2, 4),\n",
       " (4, 2),\n",
       " (4, 6),\n",
       " (6, 4),\n",
       " (6, 3),\n",
       " (3, 6)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate center-context pairs with window size 1\n",
    "\n",
    "pairs = []\n",
    "\n",
    "for sent in tokenized:\n",
    "    for i, center in enumerate(sent):\n",
    "        center_idx = word2idx[center]\n",
    "        for j in [i - 1, i + 1]:\n",
    "            if 0 <= j < len(sent):\n",
    "                context_idx = word2idx[sent[j]]\n",
    "                pairs.append((center_idx, context_idx))\n",
    "\n",
    "pairs[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "549f1bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.output = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x) # this is what we care about after training\n",
    "        x = self.output(x) # the output is the logits for each word in vocab (the probabilities that each word is next to the center word)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0e673174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss 46.3413\n",
      "Epoch 100, loss 15.1339\n",
      "Epoch 200, loss 14.8288\n",
      "Epoch 300, loss 14.7606\n",
      "Epoch 400, loss 14.7320\n",
      "Epoch 500, loss 14.7163\n",
      "Epoch 600, loss 14.7062\n",
      "Epoch 700, loss 14.6987\n",
      "Epoch 800, loss 14.6927\n",
      "Epoch 900, loss 14.6882\n",
      "Epoch 1000, loss 14.6846\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 5\n",
    "model = Word2Vec(vocab_size, embedding_dim)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "for epoch in range(1001):\n",
    "    total_loss = 0\n",
    "    for center, context in pairs:\n",
    "        center = torch.tensor([center])\n",
    "        context = torch.tensor([context])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(center)\n",
    "        loss = loss_fn(logits, context)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, loss {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7b1ff010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.0135,  5.5560,  5.5990,  1.2117, -4.2410],\n",
      "        [-2.8657, -3.0508, -0.8215, -0.6204,  1.7253],\n",
      "        [-1.6408,  3.1619,  0.9434,  1.6754, -0.0948],\n",
      "        [ 2.5798,  3.2396,  2.3454, -7.0850,  2.2606],\n",
      "        [-0.1747, -1.2600,  3.4400, -0.2828,  1.6219],\n",
      "        [ 0.9783,  1.0156,  2.6860, -0.1128,  4.1431],\n",
      "        [ 0.5452,  0.2335, -2.6784,  1.4007, -1.9947],\n",
      "        [-3.3606,  0.9111, -1.7885, -1.1207, -1.9609]])\n",
      "love vs hate: 0.24068935215473175\n",
      "nlp vs machine: 0.3324061632156372\n",
      "machine vs learning: -0.5725799202919006\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.embeddings.weight.detach()\n",
    "print(embeddings)\n",
    "\n",
    "def similarity(w1, w2):\n",
    "    v1 = embeddings[word2idx[w1]]\n",
    "    v2 = embeddings[word2idx[w2]]\n",
    "    return torch.cosine_similarity(v1, v2, dim=0).item()\n",
    "\n",
    "print(\"love vs hate:\", similarity(\"love\", \"hate\"))\n",
    "print(\"nlp vs machine:\", similarity(\"nlp\", \"machine\"))\n",
    "print(\"machine vs learning:\", similarity(\"machine\", \"learning\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e9de2b",
   "metadata": {},
   "source": [
    "## Contextual Word Embeddings (BERT, GPT)\n",
    "\n",
    "Unlike static word embeddings, contextual word embeddings assign a vector to each word based on its **surrounding context**. This means the same word can have different vector representations depending on how it is used in a sentence.\n",
    "\n",
    "These embeddings are produced by Transformer-based models such as BERT and GPT, which process entire sequences at once and use attention mechanisms to model relationships between words. As a result, the embeddings capture both word meaning and context, including syntax and semantics (homonyms are capured)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91921348",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "After generating many features, it’s important to identify the ones that actually matter. This reduces computation and can improve performance by removing noise from irrelevant or redundant features.\n",
    "\n",
    "The goal is to eliminate:\n",
    "\n",
    "- **Irrelevant features**. They contribute no useful signal\n",
    "\n",
    "- **Redundant features**. They overlap with others and add unnecessary complexity\n",
    "\n",
    "Common approaches include:\n",
    "\n",
    "1) Filter methods: rank features using statistical criteria\n",
    "\n",
    "2) Wrapper methods: evaluate subsets using a predictive model\n",
    "\n",
    "3) Embedded methods: select features during model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdcac51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.feature_selection import RFE, SequentialFeatureSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acbab89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original features: 4\n"
     ]
    }
   ],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "print(\"original features:\", X.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa26582",
   "metadata": {},
   "source": [
    "### 1) Filter methods\n",
    "\n",
    "Rank features using statistical criteria. For example how much the individual features correlate with the label.\n",
    "\n",
    "Pros\n",
    "\n",
    "- Fast and computationally cheap\n",
    "\n",
    "- Model-agnostic\n",
    "\n",
    "- Good first step for high-dimensional data (e.g., text, genomics)\n",
    "\n",
    "Cons\n",
    "\n",
    "- Consider each feature independently (ignores interactions)\n",
    "\n",
    "- May keep features that look good statistically but don’t help the final model\n",
    "\n",
    "- Can drop features that only work well in combination with others\n",
    "\n",
    "**Code example:**\n",
    "\n",
    "Chi-squared and Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8a478e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi2 scores: [ 10.81782088   3.7107283  116.31261309  67.0483602 ]\n",
      "Selected features using chi2: [0 2 3]\n"
     ]
    }
   ],
   "source": [
    "selector = SelectKBest(score_func=chi2, k=3)\n",
    "X_chi2 = selector.fit_transform(X, y)\n",
    "\n",
    "print(\"Chi2 scores:\", selector.scores_)\n",
    "print(\"Selected features using chi2:\", selector.get_support(indices=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70f5df94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MI scores: [0.48505954 0.2723968  0.9921889  0.99264549]\n",
      "Selected features using MI: [0 2 3]\n"
     ]
    }
   ],
   "source": [
    "mi_scores = mutual_info_classif(X, y)\n",
    "\n",
    "k = 3\n",
    "top_indices = mi_scores.argsort()[-k:]\n",
    "\n",
    "print(\"MI scores:\", mi_scores)\n",
    "print(\"Selected features using MI:\", top_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a739c2c",
   "metadata": {},
   "source": [
    "### 2) Wrapper Methods\n",
    "\n",
    "Rank or select features by repeatedly training a model on different feature subsets. The model’s performance guides which features are kept.\n",
    "\n",
    "Pros\n",
    "\n",
    "- Captures interactions between features\n",
    "\n",
    "- Tailored to a specific model and objective\n",
    "\n",
    "Cons\n",
    "\n",
    "- Computationally expensive (many model evaluations)\n",
    "\n",
    "- Can overfit when data is limited\n",
    "\n",
    "**Examples**:\n",
    "\n",
    "Recursive Feature Elimination (RFE)\n",
    "\n",
    "- Starts with all features, trains a model, removes the weakest feature, and repeats until the desired number remains.\n",
    "\n",
    "Sequential Feature Selection (SFS)\n",
    "\n",
    "- Adds (forward) or removes (backward) one feature at a time, choosing whichever improves model performance the most.\n",
    "\n",
    "**Code example:**\n",
    "\n",
    "Logistic Regression with RFE and SFS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2485ac82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected feature indices (RFE): [1 2 3]\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=1000)\n",
    "rfe = RFE(estimator=model, n_features_to_select=3)\n",
    "rfe.fit(X, y)\n",
    "\n",
    "print(\"Selected feature indices (RFE):\", rfe.get_support(indices=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de40cd8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected feature indices (SFS): [0 2 3]\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "sfs = SequentialFeatureSelector(\n",
    "    estimator=model,\n",
    "    n_features_to_select=3,\n",
    "    direction=\"forward\" # or \"backward\"\n",
    ")\n",
    "sfs.fit(X, y)\n",
    "\n",
    "print(\"Selected feature indices (SFS):\", sfs.get_support(indices=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80f0cdd",
   "metadata": {},
   "source": [
    "When you start with many features but want only a few, forward selection is more practical (small search path).\n",
    "\n",
    "When you start with many features and want most of them, backward methods make more sense (less to remove)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b010eea",
   "metadata": {},
   "source": [
    "### 3) Embedded Methods\n",
    "\n",
    "Select features as part of the model training process. The model itself decides which features matter by shrinking coefficients (L1 regularization) or assigning importance scores (tree-based models).\n",
    "\n",
    "Pros\n",
    "\n",
    "- Feature selection is integrated into training\n",
    "\n",
    "- Can capture interactions and complex patterns\n",
    "\n",
    "Cons\n",
    "\n",
    "- Model-dependent (not transferable across architectures)\n",
    "\n",
    "- Regularization strength must be tuned in the case of Lasso\n",
    "\n",
    "**Code example:**\n",
    "\n",
    "Random Forest Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc820a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 1]\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1][:k]  # descending order\n",
    "\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de61b83",
   "metadata": {},
   "source": [
    "### A Concrete example: Activity Recognition with Smartphone Accelerometer Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b222c9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   tBodyAcc-mean()-X  tBodyAcc-mean()-Y  tBodyAcc-mean()-Z  tBodyAcc-std()-X  \\\n",
      "0           0.288585          -0.020294          -0.132905         -0.995279   \n",
      "1           0.278419          -0.016411          -0.123520         -0.998245   \n",
      "2           0.279653          -0.019467          -0.113462         -0.995380   \n",
      "3           0.279174          -0.026201          -0.123283         -0.996091   \n",
      "4           0.276629          -0.016570          -0.115362         -0.998139   \n",
      "\n",
      "   tBodyAcc-std()-Y  tBodyAcc-std()-Z  tBodyAcc-mad()-X  tBodyAcc-mad()-Y  \\\n",
      "0         -0.983111         -0.913526         -0.995112         -0.983185   \n",
      "1         -0.975300         -0.960322         -0.998807         -0.974914   \n",
      "2         -0.967187         -0.978944         -0.996520         -0.963668   \n",
      "3         -0.983403         -0.990675         -0.997099         -0.982750   \n",
      "4         -0.980817         -0.990482         -0.998321         -0.979672   \n",
      "\n",
      "   tBodyAcc-mad()-Z  tBodyAcc-max()-X  ...  fBodyBodyGyroJerkMag-kurtosis()  \\\n",
      "0         -0.923527         -0.934724  ...                        -0.710304   \n",
      "1         -0.957686         -0.943068  ...                        -0.861499   \n",
      "2         -0.977469         -0.938692  ...                        -0.760104   \n",
      "3         -0.989302         -0.938692  ...                        -0.482845   \n",
      "4         -0.990441         -0.942469  ...                        -0.699205   \n",
      "\n",
      "   angle(tBodyAccMean,gravity)  angle(tBodyAccJerkMean),gravityMean)  \\\n",
      "0                    -0.112754                              0.030400   \n",
      "1                     0.053477                             -0.007435   \n",
      "2                    -0.118559                              0.177899   \n",
      "3                    -0.036788                             -0.012892   \n",
      "4                     0.123320                              0.122542   \n",
      "\n",
      "   angle(tBodyGyroMean,gravityMean)  angle(tBodyGyroJerkMean,gravityMean)  \\\n",
      "0                         -0.464761                             -0.018446   \n",
      "1                         -0.732626                              0.703511   \n",
      "2                          0.100699                              0.808529   \n",
      "3                          0.640011                             -0.485366   \n",
      "4                          0.693578                             -0.615971   \n",
      "\n",
      "   angle(X,gravityMean)  angle(Y,gravityMean)  angle(Z,gravityMean)  subject  \\\n",
      "0             -0.841247              0.179941             -0.058627        1   \n",
      "1             -0.844788              0.180289             -0.054317        1   \n",
      "2             -0.848933              0.180637             -0.049118        1   \n",
      "3             -0.848649              0.181935             -0.047663        1   \n",
      "4             -0.847865              0.185151             -0.043892        1   \n",
      "\n",
      "   Activity  \n",
      "0  STANDING  \n",
      "1  STANDING  \n",
      "2  STANDING  \n",
      "3  STANDING  \n",
      "4  STANDING  \n",
      "\n",
      "[5 rows x 563 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7352 entries, 0 to 7351\n",
      "Columns: 563 entries, tBodyAcc-mean()-X to Activity\n",
      "dtypes: float64(561), int64(1), object(1)\n",
      "memory usage: 31.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./data/train.csv\")\n",
    "\n",
    "print(df.head())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56f6e51",
   "metadata": {},
   "source": [
    "The dataset contains 561 features, which makes model training unnecessarily heavy. To speed up computation and improve interpretability, it makes sense to reduce the feature set. For this example, I limit the number of features to 20.\n",
    "\n",
    "Because of the large feature count, backward wrapper methods (like RFE or SBS) would be too computationally expensive. Instead, I use Random Forest-based feature importances, an embedded method that identifies useful features directly from the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "610c7ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features accuracy: 0.9809653297076818\n",
      "Top 20 features accuracy: 0.973487423521414\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "n_features = 20\n",
    "\n",
    "df = pd.read_csv(\"./data/train.csv\").drop(columns=\"subject\")\n",
    "df[\"Activity\"] = LabelEncoder().fit_transform(df[\"Activity\"])\n",
    "X, y = df.drop(\"Activity\", axis=1), df[\"Activity\"]\n",
    "\n",
    "# Train random forest to get feature importances\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "rf.fit(X, y)\n",
    "top20 = rf.feature_importances_.argsort()[-n_features:][::-1]\n",
    "selected_features = X.columns[top20]\n",
    "\n",
    "# Evaluate model with all features\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "print(\"All features accuracy:\", accuracy_score(y_test, rf.predict(X_test)))\n",
    "\n",
    "# Evaluate model with top 20 features\n",
    "X_sel = X[selected_features]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sel, y, test_size=0.2, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "print(\"Top 20 features accuracy:\", accuracy_score(y_test, rf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c63d37",
   "metadata": {},
   "source": [
    "The performance remains nearly identical (accuracy 0.981 vs. 0.973) even after reducing the feature set from 561 to 20, showing that only a small subset of features carries most of the predictive power.\n",
    "Random forest feature importances provided a fast and effective way to identify these, avoiding the heavy computation cost of wrapper methods like RFE or SBS while still accounting for feature interactions and nonlinearity.\n",
    "\n",
    "(note that feature selection was done before splitting to simplify the example.\n",
    "In a full analysis, it should be performed using only the training data to avoid data leakage.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
